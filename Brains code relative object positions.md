The relative position of an object can be coded in various coordinate frames, such as relative to the eyes or the body. Reaching plans seem coded in eye coordinates because they appear relative to the eye fixation position. When moving the eye fixation, reaching plans change in coding. When moving the initial hand location, reaching plans don't change. PRR neurons code for the position of the target of the reaching movement in eye coordinates. Remapping consists of changing object location representation based on changed eye attending location. In the intervening saccade task, the monkey has to move her eyes while keeping track of the target location. Neurons whose receptive fields cover the target after the intervening sacade start firing for remapping. Visual targets of reaching movements are coded in eye coordinates. If we locate the object in eye coordinates, then how do we actually grasp it? The first hypothesis claims that info in eye coordinates is translated to head coordinates, body coordinates, and hand coordinates. The second hypothesis claims that all body shape information gets combined in one go. The third hypothesis claims that info in eye coordinates is directly translated to hand coordinates. By probing neurons involved in movement intention we can predict future movements before they occur and use this in building brain-computer interfaces. The reason why PPC is probed in BCI's, instead of the motor cortex, is it codes intentions and high-level planning of a subject, which are a more useful basis for robotic movement. The intervening saccade task shows that the activity in PRR shifted to update the target location in light of new eye position. Three different models propose different ways for how info in eye coordinates gets translated to hand coordinates for reaching.