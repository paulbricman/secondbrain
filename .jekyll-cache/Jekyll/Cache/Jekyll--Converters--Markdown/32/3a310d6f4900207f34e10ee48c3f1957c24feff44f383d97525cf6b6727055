I"[<p>A machine learning model can reach a region of the <a class="internal-link" href="/secondbrain/argmin-and-argmax-formalize-optimization">optimization space</a> where its performance as judged by the optimization function is superb, yet its behavior with respect to its creator’s intentions is deplorable. For instance, an agent in a simulated physical environment may learn to exploit a bug of the physics engine, scoring high with a meaningless strategy, because it <a class="internal-link" href="/secondbrain/biological-reward-systems-are-malleable">hijacked its own reward system</a>. Similarly, addicted individuals, together with their internal reward systems, are learning to optimize for innate drives, but don’t end up manifesting meaningful behavior. Addiction arguably leads to a counterproductive behavior for the individual and society, even if performance on the dopamine optimization function is superb. Therefore, in an <a class="internal-link" href="/secondbrain/argmin-and-argmax-formalize-optimization">optimization setting</a>, any dimension not captured by the <a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">optimization function</a> will be ignored in pursuit of higher formalized performance (i.e. the “shortcut rule”). That said, generality and novelty might be fool-proof optimization targets.</p>
:ET