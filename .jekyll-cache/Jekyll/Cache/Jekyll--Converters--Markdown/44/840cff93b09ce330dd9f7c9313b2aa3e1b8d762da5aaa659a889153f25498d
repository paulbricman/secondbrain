I"Q<p>In <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">machine learning</a>, one often fears that <a class="internal-link" href="/secondbrain/regularization-penalizes-flexibility">models learn to memorize training data</a>, and simply regurgitate it during training in one way or another. This criticism has been widespread for <a class="internal-link" href="/secondbrain/language-models-are-few-shot-learners">few-shot generalization in language models</a>. In this, large models which absorb more knowledge can be seen as possessing more <a class="internal-link" href="/secondbrain/cognitive-functions-decline-with-age">crystallized intelligence</a> than fluid intelligence. How can we <a class="internal-link" href="/secondbrain/wireheading-is-like-addiction">optimize for fluid intelligence</a>?</p>
:ET