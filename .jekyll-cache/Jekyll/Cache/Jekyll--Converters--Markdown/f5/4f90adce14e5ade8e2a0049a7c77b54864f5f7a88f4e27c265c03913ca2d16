I"<p>When you <a class="internal-link" href="/secondbrain/medium-influences-message">exchange</a> <a class="internal-link" href="/secondbrain/electromyography-enables-covert-speech-recognition">voice</a> messages on an instant messaging platform, you have to <a class="internal-link" href="/secondbrain/biochemical-signaling-is-like-speaking">listen</a> to the whole message before sending your own. However, on a voice call, you spontaneously contribute as the <a class="internal-link" href="/secondbrain/conversational-interfaces-render-knowledge-bases-into-agents">conversation</a> is flowing. In this, <a class="internal-link" href="/secondbrain/vanilla-rnns-often-run-into-vanishing-and-exploding-gradients">RNNs</a> work by similarly first encoding all <a class="internal-link" href="/secondbrain/ds-can-be-entrained-by-input">input</a> <a class="internal-link" href="/secondbrain/information-hygiene-is-like-spacecraft-sanitization">information</a> in an <a class="internal-link" href="/secondbrain/embeddings-in-machine-learning-represent-prototypes-or-exemplars">embedding</a>, before unpacking that into an answer. In contrast, transformers build an answer as the <a class="internal-link" href="/secondbrain/streams-formalize-input-output">input stream</a> progresses.</p>
:ET