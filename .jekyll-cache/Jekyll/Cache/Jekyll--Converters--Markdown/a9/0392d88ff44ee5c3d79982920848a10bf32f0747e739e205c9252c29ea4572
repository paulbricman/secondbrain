I"p<p>If the <a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">risk function</a> is differentiable, as it is often the case in <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">machine learning</a>, then <a class="internal-link" href="/secondbrain/momentum-in-optimization-is-trend-observation-in-retrospective-futurism">optimization</a> can be performed by gradient descent. Essentially, the model parameters are tweaked so that the <a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">risk</a> gets minimized.</p>
:ET