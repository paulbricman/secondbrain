I"<p>Contrary to what many believe, reaching the global optimum through <a class="internal-link" href="/secondbrain/end-to-end-differentiation-enables-powerful-optimization-techniques">gradient descent</a> is NOT the goal of <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">supervised learning</a>. Reaching the global minimum of the <a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">empirical risk surface</a> would virtually guarantee <a class="internal-link" href="/secondbrain/regularization-penalizes-flexibility">overfitting</a>. Rather, local minima are more than enough, stopping when the <a class="internal-link" href="/secondbrain/regularization-penalizes-flexibility">testing loss starts to increase</a>.</p>
:ET