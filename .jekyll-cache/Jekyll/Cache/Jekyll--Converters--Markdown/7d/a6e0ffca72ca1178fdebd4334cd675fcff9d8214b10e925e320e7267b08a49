I"<p>When using <a class="internal-link" href="/secondbrain/backpropagation-through-time-makes-rnn-s-feasible">backpropagation through time</a> with <a class="internal-link" href="/secondbrain/vanilla-rnn-s-often-run-into-vanishing-and-exploding-gradients">RNNâ€™s</a>, <a class="internal-link" href="/secondbrain/training-and-inference-maintain-and-change-different-values">all weights are learned</a>. In contrast, in reservoir computing, only the output weights which read off the harvested <a class="internal-link" href="/secondbrain/neural-dynamicity-has-evolutionary-advantages">dynamics</a> are learned through a <a class="internal-link" href="/secondbrain/argmin-and-argmax-formalize-optimization">closed-form</a> linear regression. In a sense, reservoir computing projects the <a class="internal-link" href="/secondbrain/ds-can-be-entrained-by-input">input</a> in a <a class="internal-link" href="/secondbrain/curse-of-dimensionality-hinders-high-dimensional-mappings">high-dimensional space</a> which is then linearly separable.</p>
:ET