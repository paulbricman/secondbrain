I"$<table>
  <tbody>
    <tr>
      <td>A machine learning model can reach a region of the [[Argmin and argmax formalize optimization</td>
      <td>optimization space]] where its performance as judged by the optimization function is superb, yet its behavior with respect to its creator’s intentions is deplorable. For instance, an agent in a simulated physical environment may learn to exploit a bug of the physics engine, scoring high with a meaningless strategy, because it [[Biological reward systems are malleable</td>
      <td>hijacked its own reward system]]. Similarly, addicted individuals, together with their internal reward systems, are learning to optimize for innate drives, but don’t end up manifesting meaningful behavior. Addiction arguably leads to a counterproductive behavior for the individual and society, even if performance on the dopamine optimization function is superb. Therefore, in an [[Argmin and argmax formalize optimization</td>
      <td>optimization setting]], any dimension not captured by the [[Risk is statistical expectation of loss</td>
      <td>optimization function]] will be ignored in pursuit of higher formalized performance (i.e. the “shortcut rule”). That said, generality and novelty might be fool-proof optimization targets.</td>
    </tr>
  </tbody>
</table>
:ET