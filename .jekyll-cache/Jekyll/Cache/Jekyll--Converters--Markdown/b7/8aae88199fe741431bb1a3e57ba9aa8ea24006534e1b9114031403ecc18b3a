I"t<p>Say youâ€™re <a class="internal-link" href="/secondbrain/personalized-language-model-perplexity-approximates-surprisal">fine-tuning</a> a <a class="internal-link" href="/secondbrain/language-models-are-few-shot-learners">language model</a> to <a class="internal-link" href="/secondbrain/perplexity-estimation-enables-antifragile-learning">estimate your perplexity</a>. However, traces from the original training data makes interpreting the resulting perplexity difficult: it might work undesirably well in various situations. Subtracting output probabilities of the <a class="internal-link" href="/secondbrain/pretrained-models-are-universal-computation-engines">pretrained model</a> from the <a class="internal-link" href="/secondbrain/personalized-language-model-perplexity-approximates-surprisal">fine-tuned model</a> might help highlight the influence of fine-tuning data.</p>
:ET