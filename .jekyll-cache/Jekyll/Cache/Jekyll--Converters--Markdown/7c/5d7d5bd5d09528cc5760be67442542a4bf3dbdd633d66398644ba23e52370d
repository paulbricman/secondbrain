I"<p>In most applications, <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">machine learning models</a>, such as <a class="internal-link" href="/secondbrain/feedforward-networks-approximate-functions">simple feedforward networks</a> perform <a class="internal-link" href="/secondbrain/few-shot-regime-enables-extreme-customization">inference</a> by <a class="internal-link" href="/secondbrain/spatial-metaphors-provide-primitives-for-latent-space-navigation">interpolating</a> the <a class="internal-link" href="/secondbrain/embeddings-in-machine-learning-represent-prototypes-informed-by-exemplars">latent space</a> derived through <a class="internal-link" href="/secondbrain/pretrained-models-are-universal-computation-engines">training</a>. Therefore, those models have trouble generalizing beyond the domain of <a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">training data</a>. Program synthesis, in contrast, attempts to generalize through extrapolation.</p>
:ET