I"v<table>
  <tbody>
    <tr>
      <td>LSTM’s mitigate some of the issues of [[Recurrent networks approximate dynamical systems</td>
      <td>RNN’s]], namely the [[Vanilla RNN’s often run into vanishing and exploding gradients</td>
      <td>vanishing and exploding gradients]], by introducing some specialized neural ensembles. Through three gates (i.e. input, forget, output), the [[Dynamical systems have a certain anatomy</td>
      <td>representations]] stored in the [[Recurrent networks approximate dynamical systems</td>
      <td>RNN]] are carefully modulated [[Dynamical systems have memory</td>
      <td>across time]]. The input gate controls the influence of the input on memory, the output gate controls how much of the memory is written to output, and the forget gate controls how much of the memory is preserved to the next step.</td>
    </tr>
  </tbody>
</table>
:ET