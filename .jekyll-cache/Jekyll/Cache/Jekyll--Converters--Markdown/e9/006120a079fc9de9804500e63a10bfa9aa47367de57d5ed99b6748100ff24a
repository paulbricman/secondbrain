I"F<p>Attention can seen as a process of <a class="internal-link" href="/secondbrain/mental-representations-connect-internal-with-external-state">representational resource</a> allocation. In this view, attention is the <a class="internal-link" href="/secondbrain/premotor-theory-of-attention-is-fundamentally-enactive">enactive</a> process of <a class="internal-link" href="/secondbrain/representationism-in-cognition-is-adaptationism-in-evolution">representing</a> using varying levels of detail. <a class="internal-link" href="/secondbrain/dynamic-attention-enables-object-permanence">Attended stimuli</a> might have <a class="internal-link" href="/secondbrain/isomorphic-representations-partially-preserve-structure">their structure better preserved, more of their variance would be explained</a>. This model is compatible with the attention formalisms used in <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">ML</a>, such as self-attention in <a class="internal-link" href="/secondbrain/language-models-are-few-shot-learners">transformers</a>.</p>
:ET