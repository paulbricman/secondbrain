I"7<p>Given unlabeled data, one can create <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">training samples</a> from by partially corrupting them and tasking a model to restore them. This has been done in NLP with BERT. The same approach is used in <a class="internal-link" href="/secondbrain/dataset-size-influences-impact-of-noise">denoising</a> autoencoders. Additionally, given unlabeled data, one can also create training samples by extracting meaningful pairs of snippets from one initial data point or two different ones, and minimizing a triplet loss. This matching procedure is present in the joint-embedding architecture with Siamese networks, like in face recognition. Those techniques have proven useful for <a class="internal-link" href="/secondbrain/self-supervised-learning-approximates-commonsense">developing models with commonsense knowledge</a>. Whatâ€™s more, those techniques could inform <a class="internal-link" href="/secondbrain/constructivism-fosters-understanding-through-scaffolding">human learning</a>.</p>
:ET