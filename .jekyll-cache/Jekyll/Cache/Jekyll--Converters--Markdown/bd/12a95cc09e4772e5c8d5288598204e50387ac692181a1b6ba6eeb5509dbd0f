I"3<p>Compared to <a class="internal-link" href="/secondbrain/feedforward-networks-approximate-functions">arbitrarily wide perceptrons</a> which describe a one-step transformation, <a class="internal-link" href="/secondbrain/deep-learning-is-consonant-with-empiricism">deep learning</a> often describes <a class="internal-link" href="/secondbrain/can-cognitive-tasks-be-factored">a series of simpler transformations which can are chained together</a>. This <a class="internal-link" href="/secondbrain/backpropagation-through-time-makes-rnn-s-feasible">makes deep learning practical</a>, even if <a class="internal-link" href="/secondbrain/feedforward-networks-approximate-functions">one-layer</a> <a class="internal-link" href="/secondbrain/perceptron-pioneers-nn-s">perceptrons</a> could always do the job in theory.</p>
:ET