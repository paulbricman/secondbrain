I"U<p>If a <a class="internal-link" href="/secondbrain/language-models-are-few-shot-learners">language model</a> has been trained, fine-tuned, or <a class="internal-link" href="/secondbrain/few-shot-regime-enables-extreme-customization">few-shot learned</a> on the entirety of documents read by a <a class="internal-link" href="/secondbrain/creative-output-can-support-educational-input-in-free-learning">learner</a>, then its perplexity in <a class="internal-link" href="/secondbrain/written-word-is-a-proxy-for-thought">predicting the contents of a text</a> are a proxy for the surprisal experienced by the <a class="internal-link" href="/secondbrain/how-could-learner-aware-tools-improve-concept-learning">learner</a>. Unpredictable passages are both surprising for the learner and perplexing for the language model. The processing fluency is low. In this case, the language model is an approximation of the learnerâ€™s own (mental) language model. Teacher-forcing might make this <a class="internal-link" href="/secondbrain/restricted-boltzmann-machine-renders-boltzmann-machine-feasible">tractable</a>.</p>
:ET