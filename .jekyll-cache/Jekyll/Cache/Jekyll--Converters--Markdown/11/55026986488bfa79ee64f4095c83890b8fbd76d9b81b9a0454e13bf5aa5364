I"«<table>
  <tbody>
    <tr>
      <td>Compared to [[Feedforward networks approximate functions</td>
      <td>arbitrarily wide perceptrons]] which describe a one-step transformation, [[Deep learning is consonant with empiricism</td>
      <td>deep learning]] often describes [[Can cognitive tasks be factored?</td>
      <td>a series of simpler chained transformations]]. This [[Backpropagation through time makes RNNs feasible</td>
      <td>makes deep learning practical]], even if [[Feedforward networks approximate functions</td>
      <td>one-layer]] [[Perceptron pioneers NNâ€™s</td>
      <td>perceptrons]] could always do the job in theory.</td>
    </tr>
  </tbody>
</table>
:ET