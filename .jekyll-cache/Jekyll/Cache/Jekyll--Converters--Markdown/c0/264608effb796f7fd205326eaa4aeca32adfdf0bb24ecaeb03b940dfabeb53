I"í<p>In <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">supervised learning</a>, simple perceptrons have been shown to be able to <a class="internal-link" href="/secondbrain/regularization-penalizes-flexibility">accurately describe an extremely broad family of functions</a>. However, they would need an extremely wide single layer to do this, sometimes even larger than there are atoms in the universe, making it <a class="internal-link" href="/secondbrain/restricted-boltzmann-machine-renders-boltzmann-machine-feasible">unfeasible to train</a>. In light of this <a class="internal-link" href="/secondbrain/deep-learning-enables-simple-chained-transformations">deep learning is more economical</a>.</p>
:ET