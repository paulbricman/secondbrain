I"ô<p>By unfolding a <a class="internal-link" href="/secondbrain/recurrent-networks-approximate-dynamical-systems">recurrent neural network</a> into a <a class="internal-link" href="/secondbrain/feedforward-networks-approximate-functions">feedforward one</a> using multiple clones for each timestamp, one can apply <a class="internal-link" href="/secondbrain/backpropagation-renders-gradient-descent-feasible">backpropagation to RNNâ€™s</a>. This makes the infamously difficult to train models somewhat easier to train. However, there are drawbacks. For instance, only a certain number of timestamps can be considered, limiting the ability of RNNâ€™s to <a class="internal-link" href="/secondbrain/dynamical-systems-have-memory">learn memory effects</a>.</p>
:ET