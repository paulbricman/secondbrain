I"®<table>
  <tbody>
    <tr>
      <td>Hopfield networks are an early attempt at building artificial [[Memory is like a manuscript</td>
      <td>memory systems]] based on [[Neural networks merge memory and computation</td>
      <td>neural networks]]. They are trained to [[Risk is statistical expectation of loss</td>
      <td>shape an energy landscape]] so that the energy of [[Dynamical systems have a certain anatomy</td>
      <td>states]] associated with [[Embeddings in machine learning represent prototypes or exemplars</td>
      <td>prototype patterns]] are located in [[Backpropagation renders gradient descent feasible</td>
      <td>local optima]]. When performing [[Memory can be content-based or location-based</td>
      <td>content-based addressing]] using a [[Recovery and matching provide self-supervised signals</td>
      <td>corrupted pattern]], the Hopfield network [[Finite-state dynamical systems are diverse</td>
      <td>dynamics]] can converge to a [[Prototypical reasoning enriches exemplars</td>
      <td>prototype]] based on [[Categorical perception translates in a blindspot for nuance</td>
      <td>its basins of attraction]].</td>
    </tr>
  </tbody>
</table>
:ET