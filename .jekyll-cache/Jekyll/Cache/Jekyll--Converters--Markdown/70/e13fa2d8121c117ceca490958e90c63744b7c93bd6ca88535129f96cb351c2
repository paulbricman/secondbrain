I"#<p>LSTM’s mitigate some of the issues of <a class="internal-link" href="/secondbrain/recurrent-networks-approximate-dynamical-systems">RNN’s</a>, namely the <a class="internal-link" href="/secondbrain/vanilla-rnn-s-often-run-into-vanishing-and-exploding-gradients">vanishing and exploding gradients</a>, by introducing some specialized neural ensembles. Through three gates (i.e. input, forget, output), the <a class="internal-link" href="/secondbrain/dynamical-systems-have-a-certain-anatomy">representations</a> stored in the <a class="internal-link" href="/secondbrain/recurrent-networks-approximate-dynamical-systems">RNN</a> are carefully modulated <a class="internal-link" href="/secondbrain/dynamical-systems-have-memory">across time</a>. The input gate controls the influence of the input on memory, the output gate controls how much of the memory is written to output, and the forget gate controls <a class="internal-link" href="/secondbrain/short-long-term-is-a-spectrum-for-memory">how much of the memory is preserved</a> to the next step.</p>
:ET