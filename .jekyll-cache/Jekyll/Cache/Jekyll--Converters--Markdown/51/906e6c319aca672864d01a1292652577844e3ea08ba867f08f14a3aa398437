I"½<table>
  <tbody>
    <tr>
      <td>By unfolding a [[Recurrent networks approximate dynamical systems</td>
      <td>recurrent neural network]] into a [[Feedforward networks approximate functions</td>
      <td>feedforward one]] using multiple clones for each timestamp, one can apply [[Backpropagation renders gradient descent feasible</td>
      <td>backpropagation to RNNâ€™s]]. This makes the infamously difficult to train models somewhat easier to train. However, there are drawbacks. For instance, only a certain number of timestamps can be considered, limiting the ability of RNNâ€™s to [[Dynamical systems have memory</td>
      <td>learn memory effects]].</td>
    </tr>
  </tbody>
</table>
:ET