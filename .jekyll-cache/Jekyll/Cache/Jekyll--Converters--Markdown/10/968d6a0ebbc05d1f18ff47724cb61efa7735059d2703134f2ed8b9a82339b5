I"·<p>Hopfield networks are an early attempt at building artificial <a class="internal-link" href="/secondbrain/memory-is-like-a-manuscript">memory systems</a> based on <a class="internal-link" href="/secondbrain/neural-networks-merge-memory-and-computation">neural networks</a>. They are trained to <a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">shape an energy landscape</a> so that the energy of <a class="internal-link" href="/secondbrain/dynamical-systems-have-a-certain-anatomy">states</a> associated with <a class="internal-link" href="/secondbrain/embeddings-in-machine-learning-represent-prototypes-or-exemplars">prototype patterns</a> are located in <a class="internal-link" href="/secondbrain/backpropagation-renders-gradient-descent-feasible">local optima</a>. When performing <a class="internal-link" href="/secondbrain/memory-can-be-content-based-or-location-based">content-based addressing</a> using a <a class="internal-link" href="/secondbrain/recovery-and-matching-provide-self-supervised-signals">corrupted pattern</a>, the Hopfield network <a class="internal-link" href="/secondbrain/finite-state-dynamical-systems-are-diverse">dynamics</a> can converge to a <a class="internal-link" href="/secondbrain/prototypical-reasoning-enriches-exemplars">prototype</a> based on <a class="internal-link" href="/secondbrain/categorical-perception-translates-in-a-blindspot-for-nuance">its basins of attraction</a>.</p>
:ET