I"¢<table>
  <thead>
    <tr>
      <th>Link</th>
      <th>Priority</th>
      <th>Notes</th>
      <th>Interval</th>
      <th>Next Rep</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a class="internal-link" href="/secondbrain/attractors-and-repellors-have-opposite-behaviors-in-dynamical-systems">Attractors and repellors have opposite behaviors in dynamical systems</a></td>
      <td>3.7</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/backpropagation-through-time-makes-rnn-s-feasible">Backpropagation through time makes RNNâ€™s feasible</a></td>
      <td>7.4</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/bifurcations-explain-discontinuous-child-development-stages">Bifurcations explain discontinuous child development stages</a></td>
      <td>11.1</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/bifurcations-slice-space-of-dynamical-systems-in-basins">Bifurcations slice space of dynamical systems in basins</a></td>
      <td>14.8</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/continuous-state-dynamical-systems-share-a-common-structure">Continuous-state dynamical systems share a common structure</a></td>
      <td>18.5</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/cosine-similarity-is-magnitude-agnostic">Cosine similarity is magnitude-agnostic</a></td>
      <td>22.2</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/curse-of-dimensionality-hinders-high-dimensional-mappings">Curse of dimensionality hinders high-dimensional mappings</a></td>
      <td>25.9</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/dataset-size-influences-impact-of-noise">Dataset size influences impact of noise</a></td>
      <td>29.6</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/deep-learning-enables-simple-chained-transformations">Deep learning enables simple chained transformations</a></td>
      <td>33.3</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/dynamical-systems-have-a-certain-anatomy">Dynamical systems have a certain anatomy</a></td>
      <td>37</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/dynamical-systems-have-memory">Dynamical systems have memory</a></td>
      <td>40.7</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/feedforward-networks-approximate-functions">Feedforward networks approximate functions</a></td>
      <td>44.4</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/finite-state-dynamical-systems-are-diverse">Finite-state dynamical systems are diverse</a></td>
      <td>48.1</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/global-minimum-is-not-the-target-in-machine-learning">Global minimum is not the target in machine learning</a></td>
      <td>51.8</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/lstm-s-mitigate-rnn-issues">LSTMâ€™s mitigate RNN issues</a></td>
      <td>55.5</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/neural-networks-are-a-massive-conceptual-gateway">Neural networks are a massive conceptual gateway</a></td>
      <td>59.2</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/one-hot-encodings-are-basis-vectors-in-type-space">One-hot encodings are basis vectors in type space</a></td>
      <td>62.9</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/post-digital-computing-promises-superb-energy-efficiency">Post-digital computing promises superb energy efficiency</a></td>
      <td>66.6</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/recurrent-networks-approximate-dynamical-systems">Recurrent networks approximate dynamical systems</a></td>
      <td>70.3</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/regularization-penalizes-flexibility">Regularization penalizes flexibility</a></td>
      <td>74</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">Risk is statistical expectation of loss</a></td>
      <td>77.7</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/structural-stability-describes-basin-of-similar-dynamical-systems">Structural stability describes basin of similar dynamical systems</a></td>
      <td>81.4</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">Supervised learning assumes underlying structure</a></td>
      <td>85.1</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/supervised-learning-is-argmin-in-hypothesis-space-of-model-architecture">Supervised learning is argmin in hypothesis space of model architecture</a></td>
      <td>88.8</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/training-and-inference-maintain-and-change-different-values">Training and inference maintain and change different values</a></td>
      <td>92.5</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/vanilla-rnn-s-often-run-into-vanishing-and-exploding-gradients">Vanilla RNNâ€™s often run into vanishing and exploding gradients</a></td>
      <td>96.2</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td><a class="internal-link" href="/secondbrain/argmin-and-argmax-formalize-optimization">Argmin and argmax formalize optimization</a></td>
      <td>0</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
  </tbody>
</table>
:ET