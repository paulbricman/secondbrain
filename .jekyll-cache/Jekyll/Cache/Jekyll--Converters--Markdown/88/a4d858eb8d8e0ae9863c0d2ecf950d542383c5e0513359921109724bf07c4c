I"$<p>The Boltzmann machine can <a class="internal-link" href="/secondbrain/deep-learning-is-consonant-with-empiricism">learn</a> arbitrary <a class="internal-link" href="/secondbrain/joint-marginal-and-conditional-probabilities-are-primitives-of-probability-theory">probability distributions</a> by means of minimizing the <a class="internal-link" href="/secondbrain/kl-divergence-formalizes-difference-between-discrete-probability-distributions">KL-divergence</a> between ground-truth data and <a class="internal-link" href="/secondbrain/neural-activity-is-like-airflow">neural activity</a> in its visibles. During inference (or “sleep”), <a class="internal-link" href="/secondbrain/clamping-neurons-fixes-their-activation">inputs are clamped</a> while the Boltzmann machine <a class="internal-link" href="/secondbrain/creativity-is-based-on-search-not-generation">confabulates</a> an output over time by <a class="internal-link" href="/secondbrain/mcmc-is-feasibe-compared-to-iid-sampling">sampling</a> the <a class="internal-link" href="/secondbrain/boltzmann-distribution-links-macrostates-with-probability-of-microstates">Boltzmann distribution</a> using the <a class="internal-link" href="/secondbrain/metropolis-sampler-is-an-elegant-mcmc-sampler">Metropolis sampler</a>. However, due to the time span necessary for inference, vanilla Boltzmann machines aren’t feasible. That said, <a class="internal-link" href="/secondbrain/restricted-boltzmann-machine-renders-boltzmann-machine-feasible">restricted Boltzmann machines</a> make them borderline tractable.</p>
:ET