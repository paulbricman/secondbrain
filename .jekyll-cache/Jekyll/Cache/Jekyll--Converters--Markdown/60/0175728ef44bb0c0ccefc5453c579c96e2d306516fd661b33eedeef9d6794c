I"Ý<table>
  <tbody>
    <tr>
      <td>By unfolding a [[Recurrent networks approximate dynamical systems</td>
      <td>recurrent neural network]] into a [[Feedforward networks approximate functions</td>
      <td>feedforward one]] by creating multiple clones for each considered timestamps, one can simply use [[Backpropagation renders gradient descent feasible</td>
      <td>backpropagation for RNNâ€™s]]. This makes those infamously difficult to train models somewhat easier to train, although there are still drawbacks. For instance, only a certain number of timestamps can be considered, limiting the ability of the RNN to [[Dynamical systems have memory</td>
      <td>learn memory effects]].</td>
    </tr>
  </tbody>
</table>
:ET