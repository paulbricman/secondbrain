I"x<p>Data augmentation seeks to derive more training samples from data. In contrast, the latent-variable architecture is based on fuzzily influencing hidden states in the latent space during prediction with actual input in order to stochastically generate output alternatives. Those output alternatives can provide a framework for expressing uncertainty in contrastive learning. As opposed to GANâ€™s, the latent-variable architecture uses input from a dataset. This is analogous to how <a class="internal-link" href="/secondbrain/dreaming-exploits-idiosyncratic-neural-dynamics">dreaming enables a form of data augmentation</a>.</p>
:ET