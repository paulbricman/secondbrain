I"V<p>When using <a class="internal-link" href="/secondbrain/backpropagation-through-time-makes-rnns-feasible">backpropagation through time</a> with <a class="internal-link" href="/secondbrain/vanilla-rnns-often-run-into-vanishing-and-exploding-gradients">RNNs</a>, <a class="internal-link" href="/secondbrain/training-and-inference-maintain-and-change-different-values">all weights are learned</a>. In contrast, in reservoir computing, only the output weights which read off the harvested <a class="internal-link" href="/secondbrain/neural-dynamicity-has-evolutionary-advantages">dynamics</a> are learned through a <a class="internal-link" href="/secondbrain/argmin-and-argmax-formalize-optimization">closed-form</a> linear regression. In a sense, reservoir computing explodes the <a class="internal-link" href="/secondbrain/ds-can-be-entrained-by-input">input</a> in a <a class="internal-link" href="/secondbrain/curse-of-dimensionality-hinders-high-dimensional-mappings">high-dimensional space</a> which is then <a class="internal-link" href="/secondbrain/perceptron-pioneers-nn-s">linearly separable</a>.</p>
:ET