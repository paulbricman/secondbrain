I"–<table>
  <thead>
    <tr>
      <th>Link</th>
      <th>Priority</th>
      <th>Notes</th>
      <th>Interval</th>
      <th>Next Rep</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>[[Dynamical systems have a certain anatomy]]</td>
      <td>37</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Dynamical systems have memory]]</td>
      <td>40.7</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Feedforward networks approximate functions]]</td>
      <td>44.4</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Finite-state dynamical systems are diverse]]</td>
      <td>48.1</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Global minimum is not the target in machine learning]]</td>
      <td>51.8</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[LSTMâ€™s mitigate RNN issues]]</td>
      <td>55.5</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Neural networks are a massive conceptual gateway]]</td>
      <td>59.2</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[One-hot encodings are basis vectors in type space]]</td>
      <td>62.9</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Post-digital computing promises superb energy efficiency]]</td>
      <td>66.6</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Recurrent networks approximate dynamical systems]]</td>
      <td>70.3</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Regularization penalizes flexibility]]</td>
      <td>74</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Risk is statistical expectation of loss]]</td>
      <td>77.7</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Structural stability describes basin of similar dynamical systems]]</td>
      <td>81.4</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Supervised learning assumes underlying structure]]</td>
      <td>85.1</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Supervised learning is argmin in hypothesis space of model architecture]]</td>
      <td>88.8</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Training and inference maintain and change different values]]</td>
      <td>92.5</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Vanilla RNNâ€™s often run into vanishing and exploding gradients]]</td>
      <td>96.2</td>
      <td>Â </td>
      <td>1</td>
      <td>1970-01-01</td>
    </tr>
    <tr>
      <td>[[Argmin and argmax formalize optimization]]</td>
      <td>0</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Attractors and repellors have opposite behaviors in dynamical systems]]</td>
      <td>3.7</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Backpropagation through time makes RNNâ€™s feasible]]</td>
      <td>7.4</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Bifurcations explain discontinuous child development stages]]</td>
      <td>11.1</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Bifurcations slice space of dynamical systems in basins]]</td>
      <td>14.8</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Continuous-state dynamical systems share a common structure]]</td>
      <td>18.5</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Cosine similarity is magnitude-agnostic]]</td>
      <td>22.2</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Curse of dimensionality hinders high-dimensional mappings]]</td>
      <td>25.9</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Dataset size influences impact of noise]]</td>
      <td>29.6</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
    <tr>
      <td>[[Deep learning enables simple chained transformations]]</td>
      <td>33.3</td>
      <td>Â </td>
      <td>2</td>
      <td>2021-05-10</td>
    </tr>
  </tbody>
</table>
:ET