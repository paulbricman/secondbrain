I"¯<p>In forward-mode differentiation, computing the partial derivative of <a class="internal-link" href="/secondbrain/deep-learning-enables-simple-chained-transformations">a single node</a> in a <a class="internal-link" href="/secondbrain/functional-programs-can-be-visualized-as-flowcharts">computational graph</a> requires computing PDs for all nodes in the graph, making <a class="internal-link" href="/secondbrain/end-to-end-differentiation-enables-powerful-optimization-techniques">gradient descent</a> <a class="internal-link" href="/secondbrain/restricted-boltzmann-machine-renders-boltzmann-machine-feasible">extremely expensive</a>. In contrast, in reverse-mode differentiation, which supports <a class="internal-link" href="/secondbrain/backpropagation-through-time-makes-rnns-feasible">backpropagation</a>, computing PDs for all nodes is done in a single sweep, by starting with the PDs of the output nodes and working backwards.</p>
:ET