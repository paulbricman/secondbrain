I"ü<table>
  <tbody>
    <tr>
      <td>In [[Supervised learning assumes underlying structure</td>
      <td>machine learning]], one often fears that [[Regularization penalizes flexibility</td>
      <td>models learn to memorize training data]], and simply regurgitate it during training in one way or another. This criticism has been widespread for [[Language models are few-shot learners</td>
      <td>few-shot generalization in language models]]. In this, large models which absorb more knowledge can be seen as possessing more [[Cognitive functions decline with age</td>
      <td>crystallized intelligence]] than fluid intelligence. How can we [[Wireheading is like addiction</td>
      <td>optimize for fluid intelligence]]?</td>
    </tr>
  </tbody>
</table>
:ET