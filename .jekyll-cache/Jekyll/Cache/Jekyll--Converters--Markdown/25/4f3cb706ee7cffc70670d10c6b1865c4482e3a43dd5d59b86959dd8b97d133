I"—<table>
  <tbody>
    <tr>
      <td>Compared to [[Feedforward networks approximate functions</td>
      <td>arbitrarily wide perceptrons]] which describe a one-step transformation, [[Deep learning is consonant with empiricism</td>
      <td>deep learning]] often describes [[Can cognitive tasks be factored?</td>
      <td>a series of simpler transformations which can are chained together]]. This [[Backpropagation through time makes RNNâ€™s feasible</td>
      <td>makes deep learning practical]], even if [[Feedforward networks approximate functions</td>
      <td>one-layer perceptrons could always do the job in theory]].</td>
    </tr>
  </tbody>
</table>
:ET