I"ê<table>
  <tbody>
    <tr>
      <td>The Kullback-Leibler divergence (or KL-divergence for short), is a measure of [[Cosine similarity is magnitude-agnostic</td>
      <td>(di)similarity]] of [[Discrete structure embeddings in continuous space are glitchy</td>
      <td>discrete]] two [[Joint, marginal, and conditional probabilities are primitives of probability theory</td>
      <td>probability distributions]]. It is an extremely popular [[Risk is statistical expectation of loss</td>
      <td>loss function]] when dealing with [[Few-shot learning exapts text generation</td>
      <td>probability distributions]].</td>
    </tr>
  </tbody>
</table>
:ET