I"¡<table>
  <tbody>
    <tr>
      <td>If you take a pretrained [[Recovery and matching provide self-supervised signals</td>
      <td>language model]] and [[No free lunch principle challenges local improvement role</td>
      <td>fine-tune]] 0.1% of its parameters on a different task with a [[Neuroplasticity enables cross-modal compensation</td>
      <td>different modality]] (e.g. vision), then the model fares extremely well. This might be explained by either a shared structure of [[Similar embodiment predicts manifold alignment</td>
      <td>natural signals]] or a shared set of [[Spatial metaphors provide primitives for latent space navigation</td>
      <td>computational primitives]] useful in any situation. In this view, pretrained models on [[Forecasting framing renders neural emulation feasible</td>
      <td>neural activity]] might help derive extremely powerful primitives.</td>
    </tr>
  </tbody>
</table>
:ET