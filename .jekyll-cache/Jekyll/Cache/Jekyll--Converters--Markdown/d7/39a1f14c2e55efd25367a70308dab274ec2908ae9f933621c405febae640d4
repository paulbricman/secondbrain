I"¶<p>In reinforcement learning, as opposed to <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">supervised learning</a>, <a class="internal-link" href="/secondbrain/localized-secretive-agents-create-buffer-zones">agents</a> are tasked with coming up with a strategy, a policy for <a class="internal-link" href="/secondbrain/darwinism-infuses-enaction-with-realism">acting in their world</a>. Often, this is done by augmenting <a class="internal-link" href="/secondbrain/finite-state-dynamical-systems-are-diverse">finite-state dynamical systems with a notion of reward</a>. This is done by attaching a reward to each <a class="internal-link" href="/secondbrain/dynamical-systems-have-a-certain-anatomy">transition</a>, and describing a successful policy one that maximizes the <a class="internal-link" href="/secondbrain/extrinsic-reward-dependence-is-unsustainable">received reward</a> in the long run.</p>
:ET