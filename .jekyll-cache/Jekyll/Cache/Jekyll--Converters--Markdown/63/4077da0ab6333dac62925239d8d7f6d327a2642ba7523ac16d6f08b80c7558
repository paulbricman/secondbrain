I"µ<table>
  <tbody>
    <tr>
      <td>Attention can seen as a process of [[Mental representations connect internal with external state</td>
      <td>representational resource]] allocation. In this view, attention is the [[Premotor theory of attention is fundamentally enactive</td>
      <td>enactive]] process of [[Representationism in cognition is adaptationism in evolution</td>
      <td>representing]] using varying levels of detail. [[Dynamic attention enables object permanence</td>
      <td>Attended stimuli]] might have [[Isomorphic representations partially preserve structure</td>
      <td>their structure better preserved, more of their variance would be explained]]. This model is compatible with the attention formalisms used in [[Supervised learning assumes underlying structure</td>
      <td>ML]], such as self-attention in [[Language models are few-shot learners</td>
      <td>transformers]].</td>
    </tr>
  </tbody>
</table>
:ET