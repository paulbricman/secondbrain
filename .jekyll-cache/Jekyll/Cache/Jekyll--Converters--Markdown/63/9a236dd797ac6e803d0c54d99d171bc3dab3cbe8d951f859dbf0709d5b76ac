I"ã<p>The Kullback-Leibler divergence (or KL-divergence for short), is a measure of <a class="internal-link" href="/secondbrain/cosine-similarity-is-magnitude-agnostic">(di)similarity</a> of <a class="internal-link" href="/secondbrain/discrete-structure-embeddings-in-continuous-space-are-glitchy">discrete</a> two <a class="internal-link" href="/secondbrain/joint-marginal-and-conditional-probabilities-are-primitives-of-probability-theory">probability distributions</a>. It is an extremely popular <a class="internal-link" href="/secondbrain/risk-is-statistical-expectation-of-loss">loss function</a> when dealing with <a class="internal-link" href="/secondbrain/few-shot-learning-exapts-text-generation">probability distributions</a>.</p>
:ET