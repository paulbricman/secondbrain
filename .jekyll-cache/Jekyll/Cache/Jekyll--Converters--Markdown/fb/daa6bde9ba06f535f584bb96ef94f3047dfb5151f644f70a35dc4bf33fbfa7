I"<p>Many NLP tasks can be framed as <a class="internal-link" href="/secondbrain/creativity-is-based-on-search-not-generation">text generation tasks</a>. If you <a class="internal-link" href="/secondbrain/recovery-and-matching-provide-self-supervised-signals">train a language model</a> to <a class="internal-link" href="/secondbrain/forecasting-forces-predictive-world-models-to-internalize-meaningful-representations">autoregressively generate text</a>, you get a model able to <a class="internal-link" href="/secondbrain/few-shot-regime-enables-extreme-customization">extrapolate on a handful of examples of a custom task</a>. This avoids the burden of <a class="internal-link" href="/secondbrain/pretrained-models-are-universal-computation-engines">fine-tuning on new tasks</a>.</p>
:ET