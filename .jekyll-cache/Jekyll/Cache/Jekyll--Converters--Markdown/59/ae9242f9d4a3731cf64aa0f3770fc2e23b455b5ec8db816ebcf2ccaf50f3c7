I"<p>If you take a pretrained <a class="internal-link" href="/secondbrain/recovery-and-matching-provide-self-supervised-signals">language model</a> and <a class="internal-link" href="/secondbrain/no-free-lunch-principle-challenges-local-improvement-role">fine-tune</a> 0.1% of its parameters on a different task with a <a class="internal-link" href="/secondbrain/neuroplasticity-enables-cross-modal-compensation">different modality</a> (e.g. vision), then the model fares extremely well. This might be explained by either a shared structure of <a class="internal-link" href="/secondbrain/similar-embodiment-predicts-manifold-alignment">natural signals</a> or a shared set of <a class="internal-link" href="/secondbrain/spatial-metaphors-provide-primitives-for-latent-space-navigation">computational primitives</a> useful in any situation. In this view, pretrained models on <a class="internal-link" href="/secondbrain/forecasting-framing-renders-neural-emulation-feasible">neural activity</a> might help derive extremely powerful primitives.</p>
:ET