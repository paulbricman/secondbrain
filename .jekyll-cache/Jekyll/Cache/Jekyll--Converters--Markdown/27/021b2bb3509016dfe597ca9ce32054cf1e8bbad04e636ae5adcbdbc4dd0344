I"<p>By unfolding a <a class="internal-link" href="/secondbrain/recurrent-networks-approximate-dynamical-systems">recurrent neural network</a> into a <a class="internal-link" href="/secondbrain/feedforward-networks-approximate-functions">feedforward one</a> by creating multiple clones for each considered timestamps, one can simply use <a class="internal-link" href="/secondbrain/backpropagation-renders-gradient-descent-feasible">backpropagation for RNNâ€™s</a>. This makes those infamously difficult to train models somewhat easier to train, although there are still drawbacks. For instance, only a certain number of timestamps can be considered, limiting the ability of the RNN to <a class="internal-link" href="/secondbrain/dynamical-systems-have-memory">learn memory effects</a>.</p>
:ET