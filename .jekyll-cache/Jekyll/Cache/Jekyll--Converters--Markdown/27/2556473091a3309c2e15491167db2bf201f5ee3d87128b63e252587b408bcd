I"L<p>A simple way to force a machine learning model to derive a meaningful representation about the world it lives in is by tasking it to predict the short-term future. If a model successfully manages to predict how its environment unfolds in the future, then it plausibly learned a great deal about its inner workings. This can be seen as a <a class="internal-link" href="/secondbrain/self-supervised-learning-approximates-commonsense">self-supervised learning paradigm</a>, together with <a class="internal-link" href="/secondbrain/recovery-and-matching-provide-self-supervised-signals">recovery and matching</a>. Additionally, attempting to predict oneâ€™s behavior might lead to a meaningful representation of their <a class="internal-link" href="/secondbrain/digital-twins-expose-physical-objects-to-digital-manipulations">behavior</a>.</p>
:ET