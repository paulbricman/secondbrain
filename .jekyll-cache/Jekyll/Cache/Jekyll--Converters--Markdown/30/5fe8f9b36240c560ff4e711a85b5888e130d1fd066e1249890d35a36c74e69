I"e<table>
  <tbody>
    <tr>
      <td>By representing a [[Tokenization for i+1 language learning is non-trivial</td>
      <td>token]] at multiple levels of abstraction concurrently, [[Attention is representational resource allocation</td>
      <td>from the fine structure to the coarse one, one can derive hierarchical representations]]. Elements which have [[Sum of elements represents their set in hyperdimensional computing</td>
      <td>a lot in common]] at coarse levels might be on the same branch, only to break off at a finer level. This approach might enable consequential explanations in [[Perception is context-dependent</td>
      <td>perception]], [[Memory is perception of structural artifacts</td>
      <td>memory]], and [[Discrete structure embeddings in continuous space are glitchy</td>
      <td>intelligence]].</td>
    </tr>
  </tbody>
</table>
:ET