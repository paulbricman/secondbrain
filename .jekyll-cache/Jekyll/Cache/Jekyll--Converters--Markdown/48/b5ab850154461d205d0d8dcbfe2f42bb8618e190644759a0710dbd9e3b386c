I":<p>As <a class="internal-link" href="/secondbrain/supervised-learning-assumes-underlying-structure">supervised learning is used to approximate an underlying mapping</a> across spaces with <a class="internal-link" href="/secondbrain/multi-modal-encoders-enable-cross-modal-guidance-in-latent-space">higher and higher dimensionality</a>, the <a class="internal-link" href="/secondbrain/supervised-learning-is-argmin-in-hypothesis-space-of-model-architecture">model</a> becomes more and more prone to <a class="internal-link" href="/secondbrain/machine-learning-models-have-fluid-and-crystallized-intelligence">overfitting</a>. Specific tricks like <a class="internal-link" href="/secondbrain/regularization-penalizes-flexibility">regularization and adapting model flexibility</a> are used to tackle this Lovecraftian issue.</p>
:ET