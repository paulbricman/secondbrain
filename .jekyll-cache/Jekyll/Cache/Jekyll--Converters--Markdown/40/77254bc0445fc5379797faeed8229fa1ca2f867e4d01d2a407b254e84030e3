I"<table>
  <tbody>
    <tr>
      <td>By representing a [[Tokenization for i+1 language learning is non-trivial</td>
      <td>token]] at multiple levels of abstraction concurrently, [[Attention is representational resource allocation</td>
      <td>from the fine structure to the coarse one, one can derive hierarchical representations]]. Elements which have [[Sum of elements represents their set in hyperdimensional computing</td>
      <td>a lot in common]] at coarse levels might be on the same branch, only to break off at a finer level. This approach might enable consequential explanations in [[Perception is context-dependent</td>
      <td>perception]], [[Memory is perception of structural artifacts</td>
      <td>memory]], and [[Discrete structure embeddings in continuous space are glitchy</td>
      <td>intelligence]], if combined with other mechanisms such as [[Memory can be content-based or location-based</td>
      <td>content-based memory addressing]]. GLOM might also be useful in a [[Multi-modal mappings generate multi-modal receptive fields</td>
      <td>multi-modal setting]], by housing [[Multi-modal representations foster understanding</td>
      <td>multi-modal percepts]] of an object at levels below its representation.</td>
    </tr>
  </tbody>
</table>
:ET