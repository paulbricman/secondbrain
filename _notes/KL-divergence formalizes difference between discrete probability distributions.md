---
resource: https://www.ai.rug.nl/minds/uploads/LN_NN_RUG.pdf
---

The Kullback-Leibler divergence (or KL-divergence for short), is a measure of [[Cosine similarity is magnitude-agnostic|(di)similarity]] of [[Discrete structure embeddings in continuous space are glitchy|discrete]] two [[Joint, marginal, and conditional probabilities are primitives of probability theory|probability distributions]]. It is an extremely popular [[Risk is statistical expectation of loss|loss function]] when dealing with [[Few-shot learning exapts text generation|probability distributions]].