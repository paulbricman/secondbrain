---
resource: https://arxiv.org/pdf/2004.10450.pdf
---

If [[Language models are few-shot learners|language models]] output either very high-likelihood or very low-likelihood [[Written word is a proxy for thought|completions]], they are rated as worse by [[Human API pushes crowdsourcing on the tech stack|humans]]. There seems to be a sweet spot of desired [[Perplexity estimation enables antifragile learning|likelihood]] in [[Few-shot learning exapts text generation|text]]. This is [[Sublime is a tool against motivated reasoning|likely]] true for machine-generated text as well as human-written text.