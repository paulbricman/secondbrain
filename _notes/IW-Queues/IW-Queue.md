---
scheduler: "afactor"
afactor: 2
interval: 1
---
| Link | Priority | Notes | Interval | Next Rep |
|------|----------|-------|---------|----------|
| [[Finite-state dynamical systems are diverse]] | 48.1 |  | 1 | 1970-01-01 |
| [[MCMC is feasibe compared to IID sampling]] | 49.98 |  | 1 | 1970-01-01 |
| [[Global minimum is not the target in machine learning]] | 51.8 |  | 1 | 1970-01-01 |
| [[LSTM's mitigate RNN issues]] | 55.5 |  | 1 | 1970-01-01 |
| [[Memory can be content-based or location-based]] | 57.12 |  | 1 | 1970-01-01 |
| [[Neural networks are a massive conceptual gateway]] | 59.2 |  | 1 | 1970-01-01 |
| [[One-hot encodings are basis vectors in type space]] | 62.9 |  | 1 | 1970-01-01 |
| [[Meshing discretizes continuous spaces]] | 64.26 |  | 1 | 1970-01-01 |
| [[Post-digital computing promises superb energy efficiency]] | 66.6 |  | 1 | 1970-01-01 |
| [[Recurrent networks approximate dynamical systems]] | 70.3 |  | 1 | 1970-01-01 |
| [[Metropolis sampler is an elegant MCMC sampler]] | 71.4 |  | 1 | 1970-01-01 |
| [[Regularization penalizes flexibility]] | 74 |  | 1 | 1970-01-01 |
| [[Risk is statistical expectation of loss]] | 77.7 |  | 1 | 1970-01-01 |
| [[Restricted Boltzmann machine renders Boltzmann machine feasible]] | 78.54 |  | 1 | 1970-01-01 |
| [[Structural stability describes basin of similar dynamical systems]] | 81.4 |  | 1 | 1970-01-01 |
| [[Supervised learning assumes underlying structure]] | 85.1 |  | 1 | 1970-01-01 |
| [[Short-long term is a spectrum for memory]] | 85.68 |  | 1 | 1970-01-01 |
| [[Supervised learning is argmin in hypothesis space of model architecture]] | 88.8 |  | 1 | 1970-01-01 |
| [[Training and inference maintain and change different values]] | 92.5 |  | 1 | 1970-01-01 |
| [[Understanding is remembering in disguise]] | 92.82 |  | 1 | 1970-01-01 |
| [[Vanilla RNN's often run into vanishing and exploding gradients]] | 96.2 |  | 1 | 1970-01-01 |
| [[Argmin and argmax formalize optimization]] | 0 |  | 8 | 2021-05-22 |
| [[Boltzmann distribution links macrostates with probability of microstates]] | 0 |  | 8 | 2021-05-22 |
| [[Attractors and repellors have opposite behaviors in dynamical systems]] | 3.7 |  | 8 | 2021-05-22 |
| [[Boltzmann machine is a universal learning machine]] | 7.14 |  | 8 | 2021-05-22 |
| [[Backpropagation through time makes RNN's feasible]] | 7.4 |  | 8 | 2021-05-22 |
| [[Bifurcations explain discontinuous child development stages]] | 11.1 |  | 8 | 2021-05-22 |
| [[Clamping neurons fixes their activation]] | 14.28 |  | 8 | 2021-05-22 |
| [[Bifurcations segment space of dynamical systems in basins]] | 14.8 |  | 8 | 2021-05-22 |
| [[Continuous-state dynamical systems share a common structure]] | 18.5 |  | 8 | 2021-05-22 |
| [[DS can be entrained by input]] | 21.42 |  | 8 | 2021-05-22 |
| [[Cosine similarity is magnitude-agnostic]] | 22.2 |  | 8 | 2021-05-22 |
| [[Curse of dimensionality hinders high-dimensional mappings]] | 25.9 |  | 8 | 2021-05-22 |
| [[Hopfield networks are energy-based memory models]] | 28.56 |  | 8 | 2021-05-22 |
| [[Dataset size influences impact of noise]] | 29.6 |  | 8 | 2021-05-22 |
| [[Deep learning enables simple chained transformations]] | 33.3 |  | 8 | 2021-05-22 |
| [[Joint, marginal, and conditional probabilities are primitives of probability theory]] | 35.7 |  | 8 | 2021-05-22 |
| [[Dynamical systems have a certain anatomy]] | 37 |  | 8 | 2021-05-22 |
| [[Dynamical systems have memory]] | 40.7 |  | 4 | 2021-05-20 |
| [[KL-divergence formalizes difference between discrete probability distributions]] | 42.84 |  | 4 | 2021-05-20 |
| [[Feedforward networks approximate functions]] | 44.4 |  | 4 | 2021-05-20 |