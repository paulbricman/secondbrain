---
resource: https://www.youtube.com/watch?v=SY5PvZrJhLE
---

Many NLP tasks can be framed as [[Creativity is based on search, not generation|text generation tasks]]. If you [[Recovery and matching provide self-supervised signals|train a language model]] to [[Forecasting forces predictive world models to internalize meaningful representations|autoregressively generate text]], you get a model able to [[Few-shot regime enables extreme customization|extrapolate on a handful of examples of a custom task]]. This avoids the burden of [[Pretrained models are universal computation engines|fine-tuning on new tasks]].