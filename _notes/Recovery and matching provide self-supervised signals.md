---
resource: https://www.youtube.com/watch?v=Ag1bw8MfHGQ
---

Given unlabeled data, one can create [[Supervised learning assumes underlying structure|training samples]] from by partially corrupting them and tasking a model to restore them. This has been done in NLP with BERT. The same approach is used in [[Dataset size influences impact of noise|denoising]] autoencoders. Additionally, given unlabeled data, one can also create training samples by extracting meaningful pairs of snippets from one initial data point or two different ones, and minimizing a triplet loss. This matching procedure is present in the joint-embedding architecture with Siamese networks, like in face recognition. Those techniques have proven useful for [[Self-supervised learning approximates commonsense|developing models with commonsense knowledge]]. What's more, those techniques could inform [[Constructivism fosters understanding through scaffolding|human learning]].