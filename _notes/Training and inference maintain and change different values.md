---
resource: https://www.ai.rug.nl/minds/uploads/LN_NN_RUG.pdf
---

While [[End-to-end differentiation enables powerful optimization techniques|training]] a [[Language models are few-shot learners|machine learning model]], the [[Regularization penalizes flexibility|training data is fixed]] and the [[Argmin and argmax formalize optimization|model parameters are changing]]. In contrast, during [[Few-shot regime enables extreme customization|inference]], the model parameters are fixed and the data itself is changing.