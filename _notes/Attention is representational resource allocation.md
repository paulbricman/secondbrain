---
resource: https://mitpress.mit.edu/books/being-no-one
---

Attention can seen as a process of [[Mental representations connect internal with external state|representational resource]] allocation. In this view, attention is the [[Premotor theory of attention is fundamentally enactive|enactive]] process of [[Representationism in cognition is adaptationism in evolution|representing]] using varying levels of detail. [[Dynamic attention enables object permanence|Attended stimuli]] might have [[Isomorphic representations partially preserve structure|their structure better preserved, more of their variance would be explained]]. This model is compatible with the attention formalisms used in [[Supervised learning assumes underlying structure|ML]], such as self-attention in [[Language models are few-shot learners|transformers]].